{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4657115b",
   "metadata": {},
   "source": [
    "# **Many-to-One LSTM for Sentiment Analysis and Text Generation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05f2378f",
   "metadata": {},
   "source": [
    "This project delves into sentiment analysis and text generation using Long Short-Term Memory (LSTM) neural networks. We'll start by training an LSTM model to analyze airline sentiments. The dataset contains sentiment labels (0 or 1) and text reviews, which we'll convert into numerical data using the bag-of-words technique. Then, we'll use a many-to-one LSTM architecture to predict sentiment labels from the text.\n",
    "\n",
    "Moving on, we'll explore text generation by training LSTMs on \"Alice's Adventures in Wonderland.\" These LSTMs will learn the text patterns to generate new text based on a given prompt, focusing on next-word prediction for coherent sentences.\n",
    "\n",
    "Throughout this project, we'll tackle challenges like the dynamic nature of language and words with similar meanings. Techniques such as entropy scaling and softmax temperature will help control text randomness.\n",
    "\n",
    "By the end of this project, you'll:\n",
    "\n",
    "## **Learning Outcomes**\n",
    "\n",
    "* Understand sentiment analysis and its significance in analyzing textual data.\n",
    "* Learn essential text preprocessing techniques.\n",
    "* Grasp the bag-of-words technique for numerical representation.\n",
    "* Implement LSTM for sentiment analysis.\n",
    "* Train and evaluate LSTM models for sentiment classification.\n",
    "* Explore text generation using LSTMs.\n",
    "* Recognize the challenges of text generation and how to address them.\n",
    "* Discover entropy scaling and softmax temperature for controlling text generation.\n",
    "* Develop skills in next-word prediction and generating coherent sentences.\n",
    "* Use this project as a foundation for further exploration in sentiment analysis, text generation, and deep learning with LSTMs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e766e1f9",
   "metadata": {},
   "source": [
    "## **Approach**\n",
    "\n",
    "### Sentiment Analysis:\n",
    "\n",
    "* **Dataset:**\n",
    "    - Acquire the airline sentiment dataset containing sentiment labels (0 or 1) and associated text reviews.\n",
    "\n",
    "* **Preprocessing:**\n",
    "    - Perform data preprocessing, including text cleaning, tokenization, and stop words removal.\n",
    "    - Transform text reviews into a bag-of-words representation.\n",
    "\n",
    "* **Many-to-One LSTM:**\n",
    "    - Employ the many-to-one LSTM architecture for training the sentiment detection model.\n",
    "    - Feed the bag-of-words representation of text reviews as input to the LSTM.\n",
    "\n",
    "* **Training:**\n",
    "    - Split the dataset into training and testing sets.\n",
    "    - Train the LSTM model using the training set.\n",
    "    - Evaluate the model's performance on the testing set.\n",
    "\n",
    "### Text Generation:\n",
    "\n",
    "* **Dataset:**\n",
    "    - Obtain the \"Alice's Adventures in Wonderland\" text dataset.\n",
    "\n",
    "* **Preparing and Structuring:**\n",
    "    - Preprocess the text data by cleaning, tokenizing, and structuring sentences and phrases.\n",
    "    - Create sequences for training.\n",
    "\n",
    "* **Many-to-One LSTM:**\n",
    "    - Implement the many-to-one LSTM architecture for text generation.\n",
    "    - Train the LSTM model using the prepared dataset.\n",
    "\n",
    "* **The Challenge of Text Generation:**\n",
    "    - Recognize the challenges in text generation, including language variability and context dependence.\n",
    "    - Acknowledge the rich diversity of words in natural language and the need for careful consideration during generation.\n",
    "\n",
    "* **Introducing Controlled Randomness with Entropy Scaling:**\n",
    "    - Explore entropy scaling to introduce controlled randomness in text generation.\n",
    "\n",
    "* **Understanding Softmax Temperature:**\n",
    "    - Introduce softmax temperature, a hyperparameter for controlling prediction randomness in LSTMs and neural networks.\n",
    "    - Explain the concept of predicting using the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a429595",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbbdc20e",
   "metadata": {},
   "source": [
    "## **Install Packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ba5fa-0e47-4d2d-bb8a-3e5ae32b3935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88470f0",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fc61e6-2e58-4038-b7d4-904a796a1062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from projectpro import save_point, checkpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d9b4a8-9643-4fc1-a87d-4814b446d231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vithi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86626d10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffe06da-9d21-4545-80d3-7f0f0b28f9e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Sentiment Detection with (Many to One) LSTM**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55c4aef7-c740-4e22-b3c7-f0c7ad800e6b",
   "metadata": {},
   "source": [
    "# Types of RNN Models\n",
    "\n",
    "In this post, we will explore different types of Recurrent Neural Network (RNN) models, which are widely used in various applications. \n",
    "\n",
    "## One-to-One Type\n",
    "\n",
    "The one-to-one RNN type is the basic structure where the model processes input data and produces a single output. This is the fundamental architecture of RNNs.\n",
    "\n",
    "## One-to-Many Type\n",
    "\n",
    "In the one-to-many type, the model takes a fixed-format input, such as an image, and generates a sequence of data as output. An excellent example of this is the implementation in image captioning applications.\n",
    "\n",
    "## Many-to-Many Type\n",
    "\n",
    "The many-to-many type receives a sequence of data as input and also generates a sequence of data as output. One common application of this type is machine translation.\n",
    "\n",
    "![Many-to-One](./images/many-to-one.png)\n",
    "\n",
    "\n",
    "*Many-to-one type* is the focus of this post. It takes a sequence of data as input and generates informative data or labels as output. This type is often used for classification tasks. For instance, it can be used to determine the sentiment of a sentence. When the model is trained with many-to-one architecture, it can predict whether a given sentence has a positive or negative sentiment.\n",
    "\n",
    "Consider the sentence: **\"This movie is interesting\"**. To classify its sentiment, word tokenization at the word level can be applied. If the sentence conveys a positive sentiment, it likely contains words like \"good.\" Thus, the model can classify this sentence as having a positive sentiment.\n",
    "\n",
    "To use this approach in an RNN model, the sentence is treated as a sequence of words (the \"many\" part) and classified into a label (the \"one\" part). This is the essence of a many-to-one type model.\n",
    "\n",
    "![Many-to-One Detail](./images/many-to-one_detail.png)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0117b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f38dea48-6cf7-4994-8c5c-d19bd0b659e9",
   "metadata": {},
   "source": [
    "## **Reading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bf1eef-09cd-4d5a-b74e-b5bfedac54b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "3                  0   \n",
       "4                  1   \n",
       "\n",
       "                                                                                                                                       text  \n",
       "0                                                                  @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "1            @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "2                                                                                   @VirginAmerica and it's a really big bad thing about it  \n",
       "3  @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA  \n",
       "4                                                           @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Dataset\n",
    "df = pd.read_csv('../data/airline_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7035f56",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c7e1285-70ca-4c4c-b907-8a0dc2051cb5",
   "metadata": {},
   "source": [
    "## Preprocessing Data for Training\n",
    "\n",
    "### Normalizing Sentences\n",
    "\n",
    "Normalizing sentences is a crucial step in text preprocessing for tasks like sentiment analysis and text generation. It involves the following actions:\n",
    "\n",
    "- **Remove Special Characters:** Special characters, such as punctuation marks, symbols, or emojis, don't contribute significantly to the sentiment or meaning of a sentence. Removing them allows us to focus on essential words and improves analysis efficiency.\n",
    "\n",
    "- **Convert to Lower Case:** Converting all text to lowercase reduces data dimensionality and ensures that words in different cases (e.g., \"good\" and \"Good\") are treated as the same word. This minimizes duplication and enhances sentiment capture.\n",
    "\n",
    "- **Uniformity and Consistency:** Normalizing sentences ensures consistent, standardized text data. This uniformity aids in creating a consistent representation of words and phrases, facilitating effective pattern recognition in subsequent models.\n",
    "\n",
    "### Removing Stop Words\n",
    "\n",
    "Stop words are common words in a language that carry little meaning in a sentence. Examples include \"a,\" \"the,\" \"and,\" \"is,\" and more. Removing stop words is important for the following reasons:\n",
    "\n",
    "- **Relevance to Specific Words:** Stop words frequently occur in text but often provide little value for sentiment analysis or text generation. Eliminating them helps focus on meaningful and informative words, enhancing context and coherence.\n",
    "\n",
    "- **Noise Reduction:** Stop words can introduce noise and unwanted variation in the text data. Their removal reduces noise, improving the signal-to-noise ratio, leading to more accurate analysis results and predictions.\n",
    "\n",
    "- **Reducing Dimensionality:** Stop words are high-frequency words that appear in nearly every sentence. Removing them decreases data dimensionality, enhancing computational efficiency during model training and inference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b126e338-4896-43cd-a4c3-2a3d8abce629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading English stop words\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3988683c-09bf-456f-a308-4ef5070dbb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process_text_data(text: str) -> str:\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and symbols using a regular expression\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Filter out stopwords (common words that may not contribute much to the text's meaning)\n",
    "    words = [w for w in words if (w not in stop)]\n",
    "\n",
    "    # Rejoin the filtered words to form a preprocessed text\n",
    "    words = ' '.join(words)\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fb0090-d947-4e5d-9bcb-935291c1ed56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plus added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica seriously would pay 30 flight seats playing really bad thing flying va</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear worm go away</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "3                  0   \n",
       "4                  1   \n",
       "\n",
       "                                                                                             text  \n",
       "0                                           virginamerica plus added commercials experience tacky  \n",
       "1  virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse  \n",
       "2                                                              virginamerica really big bad thing  \n",
       "3            virginamerica seriously would pay 30 flight seats playing really bad thing flying va  \n",
       "4                                     virginamerica yes nearly every time fly vx ear worm go away  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(pre_process_text_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa49121",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d978232-d848-43f3-a48a-6cac90c18f9a",
   "metadata": {},
   "source": [
    "## **Checking most used words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b14a6004-9fcf-45ea-9623-aff5bd10a3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Counter object to count word occurrences in text reviews\n",
    "counts = Counter()\n",
    "\n",
    "# Loop through the text reviews in the DataFrame and update the word counts\n",
    "for i, review in enumerate(df['text']):\n",
    "    counts.update(review.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd30f06-a179-43cd-b8a5-0aeab410b7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united',\n",
       " 'flight',\n",
       " 'usairways',\n",
       " 'americanair',\n",
       " 'southwestair',\n",
       " 'jetblue',\n",
       " 'get',\n",
       " 'cancelled',\n",
       " 'thanks',\n",
       " 'service']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the words by their frequencies in descending order\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Select and print the top 10 most frequent words\n",
    "top_10_words = words[0:10]\n",
    "print(top_10_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bacec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb92d579-609b-4636-b199-4e1a728e7046",
   "metadata": {},
   "source": [
    "## Creating Numeric Representation of Words (Bag of Words)\n",
    "\n",
    "The bag-of-words representation is a technique used to convert text data into a numerical format suitable for machine learning algorithms. It focuses on word frequency, ignoring word order and structure in a sentence. Here's an overview of the steps involved in creating a bag-of-words representation:\n",
    "\n",
    "1. **Tokenization:** The text is split into individual words or tokens, where each token represents a unit of meaning, such as a word or a combination of words.\n",
    "\n",
    "2. **Vocabulary Creation:** All unique tokens in the text data are collected to create a vocabulary or dictionary. This vocabulary represents the set of possible features for the bag-of-words representation.\n",
    "\n",
    "3. **Frequency Count:** For each document or sentence in the text data, the frequency of each token in the vocabulary is counted. This creates a numerical representation, where each entry represents the frequency of a specific token in a particular document.\n",
    "\n",
    "4. **Checking Sequence Length and Padding:** Sentence lengths may vary. To address this, the sequence length is checked, and padding is applied to ensure all sequences have the same length.\n",
    "\n",
    "The resulting bag-of-words representation is usually a matrix where each row represents a document or sentence, and each column corresponds to a token in the vocabulary. The values in the matrix indicate the frequency of each token in the respective document.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "| Document  | Word1 | Word2 | Word3 | Word4 | Word5 |\n",
    "|-----------|-------|-------|-------|-------|-------|\n",
    "| Sentence1 | 2     | 1     | 0     | 1     | 0     |\n",
    "| Sentence2 | 0     | 1     | 1     | 0     | 1     |\n",
    "| Sentence3 | 1     | 0     | 0     | 1     | 1     |\n",
    "| Sentence4 | 0     | 1     | 0     | 0     | 0     |\n",
    "\n",
    "In this example, there are four sentences (Sentence1, Sentence2, Sentence3, and Sentence4) with five unique words (Word1, Word2, Word3, Word4, and Word5). The table represents the bag-of-words representation, with each entry indicating the frequency of the corresponding word in the respective sentence. For example, in Sentence1, Word1 occurs twice, Word2 occurs once, and so on.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3ca371c-e4de-4f64-89f5-61108c021809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary that maps words to unique integer values, starting from 1\n",
    "word_to_int = {word: i for i, word in enumerate(words, start=1)}\n",
    "\n",
    "# Create a dictionary that maps integer values to words\n",
    "int_to_word = {i: word for i, word in enumerate(words, start=1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6dea37-2c71-44ce-8402-8c1cc5238ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_int(text: str, word_to_int: dict):\n",
    "    # Split the input text into words and convert each word to its corresponding integer using the word_to_int dictionary\n",
    "    return [word_to_int[word] for word in text.split()]\n",
    "\n",
    "def int_to_text(int_arr, int_to_word: dict):\n",
    "    # Join the list of integers into a string of words, using the int_to_word dictionary to map integers back to words\n",
    "    return ' '.join([int_to_word[index] for index in int_arr if index != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a158248-7fe8-43c2-9ef6-84f7e10b3e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the mapped reviews\n",
    "mapped_reviews = []\n",
    "\n",
    "# Loop through the text reviews in the DataFrame and convert each review to a list of integers using text_to_int function\n",
    "for review in df['text']:\n",
    "    mapped_reviews.append(text_to_int(review, word_to_int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3aa841-1724-4750-9c53-87eb2a34a5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: virginamerica plus added commercials experience tacky\n",
      "Mapped text: [44, 450, 1142, 2233, 100, 5429]\n"
     ]
    }
   ],
   "source": [
    "# Print the original text from the DataFrame at index 0\n",
    "print(f'Original text: {df.loc[0][\"text\"]}')\n",
    "\n",
    "# Print the mapped version of the text from the first review\n",
    "print(f'Mapped text: {mapped_reviews[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eec94556-0a3f-40d9-ac69-6bae85c21bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the lengths of mapped reviews\n",
    "length_sent = []\n",
    "\n",
    "# Loop through the mapped reviews and calculate the length (number of words) of each review\n",
    "for i in range(len(mapped_reviews)):\n",
    "    length_sent.append(len(mapped_reviews[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68d664",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d75bf5-0024-4998-80b5-471af44be311",
   "metadata": {},
   "source": [
    "## **Checking sequence length and padding accordingly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf03aea-686d-43a5-87a7-4e87acb094f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the sequence_length as the maximum length among all mapped reviews\n",
    "sequence_length = max(length_sent)\n",
    "\n",
    "# Use the pad_sequences function to pad the mapped_reviews\n",
    "X = pad_sequences(maxlen=sequence_length, sequences=mapped_reviews, padding=\"post\", value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "918a9dc5-55c7-4edc-b8d0-37fb34aafa53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  44,  450, 1142, 2233,  100, 5429,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a62fc56-7eb0-4f9d-ac69-76f6bf3e466d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835c77a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dccdd1fc-aa41-44d4-966e-92e25d20df3f",
   "metadata": {},
   "source": [
    "## **Creating LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a94d87-19a4-49a3-9478-4254fada66e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the embedding vector length to 32\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "# Set the maximum review length to 26\n",
    "max_review_length = 26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b21f4b95-583e-4985-af99-5365223129b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer with an input dimension of 12533, output dimension of 32, and input length of 26\n",
    "model.add(Embedding(input_dim=12533, output_dim=32, input_length=26))\n",
    "\n",
    "# Add the first LSTM layer with 40 units and return sequences (used for stacking LSTM layers)\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "\n",
    "# Add the second LSTM layer with 40 units and return sequences set to False (final LSTM layer)\n",
    "model.add(LSTM(40, return_sequences=False))\n",
    "\n",
    "# Add a Dense layer with 2 units and softmax activation (common for binary classification)\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical cross-entropy loss and the Adam optimizer, and measure accuracy during training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ee269b2-3df0-4371-aa7a-a70c225ec7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 26, 32)            401056    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 26, 40)            11680     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 40)                12960     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 82        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 425,778\n",
      "Trainable params: 425,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd29197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c65953e9-6e92-41ad-820b-3b5020245f9f",
   "metadata": {},
   "source": [
    "## **Preparing data for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9848c86-31b7-4f36-ae64-cc4468f90071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c36922c-6f1e-41bd-ae76-30e4136c56ff",
   "metadata": {},
   "source": [
    "## **One-hot-encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50ddca1a-8d90-4f5b-957f-b26cb85f0539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "220f6659-5c51-4a63-9b58-27d6591271bf",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18a3b18c-aa25-48ff-b59a-4c8a5795216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "253/253 [==============================] - 7s 18ms/step - loss: 0.3540 - accuracy: 0.8581 - val_loss: 0.2151 - val_accuracy: 0.9090\n",
      "Epoch 2/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.1457 - accuracy: 0.9495 - val_loss: 0.2179 - val_accuracy: 0.9194\n",
      "Epoch 3/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0763 - accuracy: 0.9752 - val_loss: 0.2963 - val_accuracy: 0.9174\n",
      "Epoch 4/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0437 - accuracy: 0.9860 - val_loss: 0.3533 - val_accuracy: 0.8908\n",
      "Epoch 5/50\n",
      "253/253 [==============================] - 4s 18ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.3947 - val_accuracy: 0.9116\n",
      "Epoch 6/50\n",
      "253/253 [==============================] - 4s 18ms/step - loss: 0.0234 - accuracy: 0.9936 - val_loss: 0.3262 - val_accuracy: 0.9012\n",
      "Epoch 7/50\n",
      "253/253 [==============================] - 4s 17ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.5257 - val_accuracy: 0.9012\n",
      "Epoch 8/50\n",
      "253/253 [==============================] - 4s 17ms/step - loss: 0.0148 - accuracy: 0.9962 - val_loss: 0.3979 - val_accuracy: 0.9012\n",
      "Epoch 9/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.5139 - val_accuracy: 0.8975\n",
      "Epoch 10/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0112 - accuracy: 0.9973 - val_loss: 0.4935 - val_accuracy: 0.8986\n",
      "Epoch 11/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.5624 - val_accuracy: 0.8952\n",
      "Epoch 12/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.4811 - val_accuracy: 0.8978\n",
      "Epoch 13/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.4964 - val_accuracy: 0.9004\n",
      "Epoch 14/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0108 - accuracy: 0.9974 - val_loss: 0.4832 - val_accuracy: 0.9047\n",
      "Epoch 15/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.6435 - val_accuracy: 0.9024\n",
      "Epoch 16/50\n",
      "253/253 [==============================] - 4s 17ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 0.5102 - val_accuracy: 0.9050\n",
      "Epoch 17/50\n",
      "253/253 [==============================] - 5s 18ms/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.6562 - val_accuracy: 0.8989\n",
      "Epoch 18/50\n",
      "253/253 [==============================] - 5s 18ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 0.5780 - val_accuracy: 0.8975\n",
      "Epoch 19/50\n",
      "253/253 [==============================] - 5s 18ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.5678 - val_accuracy: 0.8943\n",
      "Epoch 20/50\n",
      "253/253 [==============================] - 5s 18ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.6605 - val_accuracy: 0.9010\n",
      "Epoch 21/50\n",
      "253/253 [==============================] - 5s 18ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.6309 - val_accuracy: 0.9012\n",
      "Epoch 22/50\n",
      "253/253 [==============================] - 5s 21ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.6379 - val_accuracy: 0.8995\n",
      "Epoch 23/50\n",
      "253/253 [==============================] - 11s 42ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 0.6425 - val_accuracy: 0.8871\n",
      "Epoch 24/50\n",
      "253/253 [==============================] - 13s 52ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.5042 - val_accuracy: 0.9027\n",
      "Epoch 25/50\n",
      "253/253 [==============================] - 13s 53ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.4995 - val_accuracy: 0.8802\n",
      "Epoch 26/50\n",
      "253/253 [==============================] - 12s 47ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.6839 - val_accuracy: 0.8975\n",
      "Epoch 27/50\n",
      "253/253 [==============================] - 4s 17ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.5074 - val_accuracy: 0.8992\n",
      "Epoch 28/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.5751 - val_accuracy: 0.9004\n",
      "Epoch 29/50\n",
      "253/253 [==============================] - 4s 16ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.6435 - val_accuracy: 0.8943\n",
      "Epoch 30/50\n",
      "253/253 [==============================] - 5s 20ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.6693 - val_accuracy: 0.8949\n",
      "Epoch 31/50\n",
      "253/253 [==============================] - 5s 21ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 0.5200 - val_accuracy: 0.8807\n",
      "Epoch 32/50\n",
      "253/253 [==============================] - 5s 19ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.5985 - val_accuracy: 0.8943\n",
      "Epoch 33/50\n",
      "253/253 [==============================] - 4s 18ms/step - loss: 0.0091 - accuracy: 0.9978 - val_loss: 0.5835 - val_accuracy: 0.8958\n",
      "Epoch 34/50\n",
      "253/253 [==============================] - 5s 19ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.6137 - val_accuracy: 0.8958\n",
      "Epoch 35/50\n",
      "253/253 [==============================] - 5s 19ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.7093 - val_accuracy: 0.8949\n",
      "Epoch 36/50\n",
      "253/253 [==============================] - 11s 46ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.4797 - val_accuracy: 0.8963\n",
      "Epoch 37/50\n",
      "253/253 [==============================] - 13s 51ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.7170 - val_accuracy: 0.8960\n",
      "Epoch 38/50\n",
      "253/253 [==============================] - 13s 51ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.6154 - val_accuracy: 0.8958\n",
      "Epoch 39/50\n",
      "253/253 [==============================] - 12s 47ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.6723 - val_accuracy: 0.8937\n",
      "Epoch 40/50\n",
      "253/253 [==============================] - 13s 50ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.5513 - val_accuracy: 0.8972\n",
      "Epoch 41/50\n",
      "253/253 [==============================] - 13s 50ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.7484 - val_accuracy: 0.8926\n",
      "Epoch 42/50\n",
      "253/253 [==============================] - 12s 49ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.6760 - val_accuracy: 0.8969\n",
      "Epoch 43/50\n",
      "253/253 [==============================] - 12s 49ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.7797 - val_accuracy: 0.8958\n",
      "Epoch 44/50\n",
      "253/253 [==============================] - 12s 48ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.7961 - val_accuracy: 0.9007\n",
      "Epoch 45/50\n",
      "253/253 [==============================] - 13s 50ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.8050 - val_accuracy: 0.8998\n",
      "Epoch 46/50\n",
      "253/253 [==============================] - 12s 49ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.7696 - val_accuracy: 0.8978\n",
      "Epoch 47/50\n",
      "253/253 [==============================] - 12s 47ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8013 - val_accuracy: 0.8981\n",
      "Epoch 48/50\n",
      "253/253 [==============================] - 12s 47ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8156 - val_accuracy: 0.8969\n",
      "Epoch 49/50\n",
      "253/253 [==============================] - 12s 49ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8213 - val_accuracy: 0.8972\n",
      "Epoch 50/50\n",
      "253/253 [==============================] - 13s 52ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8165 - val_accuracy: 0.9007\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the training data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc89ea9-5ece-44ea-80bd-2ce4157a0da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Extract the training history from the 'history' object\n",
    "history_dict = history.history\n",
    "\n",
    "# Extract the training and validation loss values\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "# Extract the training and validation accuracy values\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "# Define the range of epochs\n",
    "epochs = range(1, len(val_loss_values) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "157d56ca-3c44-42a9-ada9-c02a3d484c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a subplot with 2 rows and 1 column, and select the first subplot (upper part)\n",
    "plt.subplot(211)\n",
    "\n",
    "# Plot the training loss as blue dots ('bo')\n",
    "plt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8ae7e0e-8999-49d4-9f62-084612262504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a subplot with 2 rows and 1 column, and select the second subplot (lower part)\n",
    "plt.subplot(212)\n",
    "\n",
    "# Plot the training accuracy as blue dots ('bo')\n",
    "plt.plot(epochs, history.history['accuracy'], 'bo', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Format the y-axis tick labels to display percentages\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x * 100) for x in plt.gca().get_yticks()])\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d7b0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81ec1114-2b17-48dc-a357-5c46673cc87c",
   "metadata": {},
   "source": [
    "## **Anecdotes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38cceeeb-b31a-41ed-ae72-b221d7d7d834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentiment(model, text) -> str:\n",
    "    # Convert the input text to a list of integers using the text_to_int function\n",
    "    text_int_embedding = text_to_int(text, word_to_int)\n",
    "    \n",
    "    # Pad the list of integers to match the sequence length\n",
    "    text_int_embedding = pad_sequences(maxlen=sequence_length, sequences=[text_int_embedding], padding=\"post\", value=0)\n",
    "    \n",
    "    # Predict the sentiment using the model and get the index of the predicted class\n",
    "    sentiment_index = np.argmax(model.predict(text_int_embedding))\n",
    "    \n",
    "    # Return the sentiment index as a string\n",
    "    return sentiment_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "729bb8a0-a6dd-4371-9727-997f9c50458e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data (X_test)\n",
    "# The result will be an array of predicted sentiment labels\n",
    "result = np.argmax(model.predict(X_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "710bcbfc-7528-4d7a-bc35-ca71913ee41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of positive sentences by iterating through the test data\n",
    "# and selecting sentences for which the model's prediction is labeled as 1 (positive sentiment)\n",
    "positive_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdef917a-c279-4d57-b11f-058560bf7f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of negative sentences by iterating through the test data\n",
    "# and selecting sentences for which the model's prediction is labeled as 0 (negative sentiment)\n",
    "negative_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec25941-c298-469a-a099-79dfdb9b7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetblue flight flight booking problems experience pretty great',\n",
       " 'southwestair leeannhealey yeah sale fares got places fly oh damn right live swa fly',\n",
       " 'jetblue news gate options',\n",
       " 'jetblue utah think thanks',\n",
       " 'united flight new york love quality planes united wtf crappy aviation newyork http co zv6cfpohl5']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6597156-a767-4065-8e23-c26d12d71590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usairways glad airline going swallowed american american always picks phone solves problems',\n",
       " 'southwestair disconnected call 2 5 hours without even speaking octaviannightmare',\n",
       " 'americanair well done taken fun air travel phlairport',\n",
       " 'usairways told coded upgrade clearly purchased seat miles refuse downgrade ripoff',\n",
       " 'rt virginamerica met match got status another airline upgrade restr http co rhkamx9vf5 http co pyalebgkjt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407805c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef70e236-27b3-4947-aaa1-7645ddb3a16b",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "### Overview\n",
    "\n",
    "To generate text, we will follow the following strategy:\n",
    "\n",
    "1. **Data Import:** We'll start by importing Project Gutenberg's dataset of \"Alice's Adventures in Wonderland,\" which can be downloaded [here](https://www.gutenberg.org/files/11/11-0.txt).\n",
    "\n",
    "2. **Data Preprocessing:** We'll preprocess the text data by converting all text to lowercase and removing punctuation.\n",
    "\n",
    "3. **Vocabulary Building:** We'll construct a vocabulary by counting the occurrences of unique words in the text. Words will be sorted based on frequency, and each word will be assigned a unique index.\n",
    "\n",
    "4. **Sequence Generation:** To train our model, we need input-output pairs in the form of sequences. We'll use a sliding window approach to create input sequences and their subsequent output words.\n",
    "\n",
    "5. **Data Encoding:** To feed data into the LSTM model, we'll encode input and output sequences. One-hot encoding will be used to represent words as binary vectors, indicating their presence in the sequence.\n",
    "\n",
    "6. **Model Architecture:** Our model will consist of an LSTM layer, a dense (fully connected) layer, and a softmax activation function. This architecture enables the model to learn patterns and generate text.\n",
    "\n",
    "7. **Training the Model:** The model will be trained using input-output pairs from the text data. It learns to minimize the difference between predicted and actual outputs using an optimizer (e.g., Adam) and a loss function (categorical cross-entropy).\n",
    "\n",
    "8. **Generating Text:** To generate text, we provide a seed sequence to the trained model. The model predicts the next word, appends it to the input sequence, and repeats the process. This iterative prediction allows us to generate text that follows the style and patterns learned during training.\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "- **Input Preparation:**\n",
    "    - Convert text to lowercase and remove non-alphanumeric characters.\n",
    "    - Split the text into individual words.\n",
    "\n",
    "- **Vocabulary Building:**\n",
    "    - Build a vocabulary by counting unique word occurrences.\n",
    "    - Assign unique indices to words.\n",
    "\n",
    "- **Sequence Generation:**\n",
    "    - Use a sliding window approach to create input-output pairs.\n",
    "    - Slide the window with a specified step size.\n",
    "\n",
    "- **Encoding the Data:**\n",
    "    - Apply one-hot encoding to represent words as binary vectors.\n",
    "    - Convert input data to a 3D array and output data to a 2D array.\n",
    "\n",
    "- **Model Architecture:**\n",
    "    - Construct a many-to-one LSTM model.\n",
    "    - Use input and LSTM layers to capture temporal dependencies.\n",
    "    - Employ a dense layer with a softmax activation function.\n",
    "\n",
    "- **Training the Model:**\n",
    "    - Train the model using input-output pairs.\n",
    "    - Minimize the difference between predicted and actual outputs.\n",
    "    - Update model parameters with an optimizer and a loss function.\n",
    "\n",
    "- **Generating Text:**\n",
    "    - Provide a seed sequence to the model.\n",
    "    - Predict the next word iteratively based on the input.\n",
    "    - Append the predicted word to the input sequence to generate subsequent words.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97dfabac-8153-47e1-8e63-5f5fb864d5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(filename: str = '../data/alice.txt'):\n",
    "    # Open the specified file for reading\n",
    "    with open(filename, encoding='utf-8-sig') as fin:\n",
    "        lines = []\n",
    "        for line in fin:\n",
    "            # Strip leading and trailing whitespace, and convert the line to lowercase\n",
    "            line = line.strip().lower()\n",
    "            \n",
    "            # Check if the line is empty, and if so, skip it\n",
    "            if (len(line) == 0):\n",
    "                continue\n",
    "            \n",
    "            # Append the processed line to the list of lines\n",
    "            lines.append(line)\n",
    "        \n",
    "        # Close the file\n",
    "        fin.close()\n",
    "        \n",
    "        # Combine the lines into a single string with spaces in between\n",
    "        text = \" \".join(lines)\n",
    "    \n",
    "    # Return the concatenated text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "924ea3e5-666e-4cc5-bd3a-f54d52ab5a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f78a57f8-779c-4b53-b0d9-78961433798e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moment to think about stopping herself before she found herself falling down a very deep well. either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. first, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[3001:3500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca9468a7-6e27-40db-9a5f-257354c5567e",
   "metadata": {},
   "source": [
    "Normalize the text to remove punctuations and convert it to lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3aaefc7c-1473-4f1b-b680-1598c9333368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process(text: str) -> str:\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphanumeric characters and replace them with spaces\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    \n",
    "    # Return the preprocessed text\n",
    "    return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6f81900-f446-4f46-8a72-97184ac0eaab",
   "metadata": {},
   "source": [
    "Assign the unique words to an index so that they can be referenced when constructing the training and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4925b6b-e392-4da8-9462-2814fa094d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Counter to count the frequency of each word in the text\n",
    "counts = Counter()\n",
    "\n",
    "# Update the counter with the words in the text (text.split() splits the text into words)\n",
    "counts.update(text.split())\n",
    "\n",
    "# Create a list of unique words (sorted by frequency, in descending order)\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Calculate the total number of words in the text\n",
    "nb_words = len(text.split())\n",
    "\n",
    "# Create a word-to-index mapping (word2index) and an index-to-word mapping (index2word)\n",
    "word2index = {word: i for i, word in enumerate(words)}\n",
    "index2word = {i: word for i, word in enumerate(words)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bb195e3-9e8a-43a8-844a-cd08fb4c081c",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b26eadbf-2f36-490f-91d0-124c697623c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97dfe608-000a-46a4-b5cb-e84608757b4f",
   "metadata": {},
   "source": [
    "Construct the input set of words that leads to an output word. Note that we are considering a sequence of 10 words and trying to predict the 11th word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f1a0752-1e80-4c00-bd24-311606840604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_and_labels(text: str, seq_length: int = SEQLEN, step: int = STEP):\n",
    "    # Initialize empty lists to store input and label sequences\n",
    "    input_words = []\n",
    "    label_words = []\n",
    "\n",
    "    # Split the input text into words\n",
    "    text_arr = text.split()\n",
    "\n",
    "    # Iterate through the text to create sequences\n",
    "    for i in range(0, nb_words - seq_length, step):\n",
    "        # Extract an input sequence of length 'seq_length'\n",
    "        x = text_arr[i:(i + seq_length)]\n",
    "        \n",
    "        # Extract the next word as the label for the input sequence\n",
    "        y = text_arr[i + seq_length]\n",
    "\n",
    "        # Append the input sequence and its corresponding label to the lists\n",
    "        input_words.append(x)\n",
    "        label_words.append(y)\n",
    "\n",
    "    return input_words, label_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c6371cc-71c4-4052-84b0-5c7febd4be19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate input and label sequences for text generation\n",
    "input_words, label_words = get_input_and_labels(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd4eca8f-3228-4d58-accd-aed4dfc810e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: the project gutenberg ebook of alice’s adventures in wonderland, by\n",
      "Output: lewis\n"
     ]
    }
   ],
   "source": [
    "# Print an example input sequence and its corresponding output label\n",
    "print(f'Input: {\" \".join(input_words[0])}\\nOutput: {label_words[0]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6337ca2-b447-4159-8482-b7bd536db594",
   "metadata": {},
   "source": [
    "Construct the vectors of the input and the output datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8c2c532-534e-47ce-988d-8eabb7beaa1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of unique words in the vocabulary\n",
    "total_words = len(set(words))\n",
    "\n",
    "# Initialize an array X for input sequences and an array y for labels\n",
    "X = np.zeros((len(input_words), SEQLEN, total_words), dtype=bool)\n",
    "y = np.zeros((len(input_words), total_words), dtype=bool)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28507c81-3a88-4c1b-ac83-b52a01a1d91b",
   "metadata": {},
   "source": [
    "We are creating empty arrays in the preceding step, which will be populated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70ca2513-711b-42da-b337-f3a0b3633dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through input sequences and labels to create one-hot encoded vectors\n",
    "for i, input_word in enumerate(input_words):\n",
    "    for j, word in enumerate(input_word):\n",
    "        # Set the corresponding position in X to 1 for the input word\n",
    "        X[i, j, word2index[word]] = 1\n",
    "        \n",
    "        # Set the corresponding position in y to 1 for the label word\n",
    "        y[i, word2index[label_words[i]]] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cc5186a-1f98-4970-9bff-7a859332dc70",
   "metadata": {},
   "source": [
    "In the preceding code, the first for loop is used to loop through all the words in the input sequence of words (10 words in input), and the second for loop is used to loop through an individual word in the chosen sequence of input words. Additionally, given that the output is a list, we do not need to update it using the second for loop (as there is no sequence of IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bb4c415-5f03-41b3-b769-a326c8151984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of X: (29584, 10, 5649)\n",
      "Input of y: (29584, 5649)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input of X: {X.shape}\\nInput of y: {y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7fd11a2-e619-4d2a-bb49-b737ada7956c",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0280a03-5b7d-4a93-ba4d-d0ec2c6aee2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants for text generation\n",
    "HIDDEN_SIZE = 128  # Hidden layer size\n",
    "BATCH_SIZE = 32  # Batch size\n",
    "NUM_ITERATIONS = 100  # Total training iterations\n",
    "NUM_EPOCHS_PER_ITERATION = 1  # Epochs per iteration\n",
    "NUM_PREDS_PER_EPOCH = 100  # Predictions per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "016337a9-2f6f-4a91-bf70-c2104887e777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 128)               2958336   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5649)              728721    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,687,057\n",
      "Trainable params: 3,687,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with specified HIDDEN_SIZE\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, total_words)))\n",
    "\n",
    "# Add a Dense layer with 'softmax' activation function\n",
    "model.add(Dense(total_words, activation='softmax')\n",
    "\n",
    "# Compile the model using 'adam' optimizer and 'categorical_crossentropy' loss\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary to display its architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d175afbf-5fd7-4683-9908-f3f1c84f87ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Fit the model**. \n",
    "- Look at how the output varies over an increasing number of epochs. \n",
    "- Generate a random set of sequences of 10 words and try to predict the next possible word. \n",
    "- We are in a position to observe how our predictions are getting better over an increasing number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff9b2d20-7abb-4ff7-8b5a-182544a11cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_model_output(model, preds: int, input_words, seq_length, total_words):\n",
    "    # Randomly select an input sequence for text generation\n",
    "    test_idx = np.random.randint(int(len(input_words) * 0.1)) * (-1)\n",
    "    test_words = input_words[test_idx]\n",
    "\n",
    "    for curr_pred in range(preds):\n",
    "        # Initialize an empty embedding for the current sequence\n",
    "        curr_embedding = np.zeros((1, seq_length, total_words))\n",
    "\n",
    "        # Set the corresponding position to 1 for each word in the input sequence\n",
    "        for i, ch in enumerate(test_words):\n",
    "            curr_embedding[0, i, word2index[ch]] = 1\n",
    "\n",
    "        # Predict the next word using the model\n",
    "        pred = model.predict(curr_embedding, verbose=0)[0]\n",
    "        word_pred = index2word[np.argmax(pred)]\n",
    "\n",
    "        # Print the generated prediction and the input sequence\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Prediction {curr_pred + 1} of {preds}\")\n",
    "        print(f'Generating from seed: {\" \".join(test_words)}\\nNext Word: {word_pred}')\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Update the input sequence with the predicted word for the next iteration\n",
    "        test_words = test_words[1:] + [word_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f7545-afb1-412a-ae50-25c5b92b3daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for iteration in range(50):\n",
    "    # Train the model using the input sequences (X) and labels (y)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, validation_split=0.1)\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "        # Generate text predictions every 10 iterations\n",
    "        check_model_output(model, 5, input_words, SEQLEN, total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef015f03-b7a0-4f6c-9f19-67ec752e3ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_word(model, input_text: str, seq_length, total_words, temperature=None):\n",
    "    # Initialize an empty embedding for the current sequence\n",
    "    curr_embedding = np.zeros((1, seq_length, total_words))\n",
    "    \n",
    "    for i, ch in enumerate(input_text):\n",
    "        # Set the corresponding position to 1 for each word in the input sequence\n",
    "        curr_embedding[0, i, word2index[ch]] = 1\n",
    "    \n",
    "    # Predict the next word using the model\n",
    "    pred = model.predict(curr_embedding, verbose=0)[0]\n",
    "    \n",
    "    if temperature is None:\n",
    "        # If no temperature is specified, select the word with the highest probability\n",
    "        word_pred = index2word[np.argmax(pred)]\n",
    "    else:\n",
    "        # If a temperature is specified, sample the next word based on the temperature\n",
    "        next_word_token = tf.random.categorical(tf.expand_dims(pred / temperature, 0), num_samples=1)[-1, 0].numpy()\n",
    "        word_pred = index2word[next_word_token]\n",
    "    \n",
    "    return pred, word_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ee59b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7609074f-d056-4020-b47a-c41d5986cd71",
   "metadata": {},
   "source": [
    "## The Challenge of Text Generation\n",
    "\n",
    "Language is a dynamic and ever-evolving system. The task of text generation in natural language processing faces several challenges due to the fluid nature of language. Key considerations include:\n",
    "\n",
    "- **Context Dependency:** The choice of the next word in a sentence heavily relies on the context established by previous words. The meaning and relevance of words are intricately tied to the preceding text.\n",
    "\n",
    "- **Variety of Vocabulary:** Natural language employs a vast and diverse vocabulary. Multiple words often exist with similar meanings, and the selection of one over the other can impact style and nuance.\n",
    "\n",
    "- **Non-Stationarity:** Language is not stationary; it evolves over time, reflecting shifts in culture, trends, and linguistic norms. Text generation models must adapt to these changes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0760f980-a1b9-4f78-898c-8dee56a291ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the people that walk with their heads downward! the\n"
     ]
    }
   ],
   "source": [
    "test_words = input_words[-28701]\n",
    "print(' '.join(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0eb6f039-3935-4d8f-a323-ee44a12fdefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict the next word using the 'model' with input 'test_words'.\n",
    "logits, word_pred = predict_next_word(model, test_words, SEQLEN, total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02743517-f9e0-42dd-a01b-b0dbc6fcf993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: time\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted word: {word_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "212b2e39-0f0b-44ee-9820-0d6aa42374bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_paragraph(model, seed, words: int, temperature: int):\n",
    "    # Initialize the 'full_text' with the 'seed' content.\n",
    "    full_text = seed.copy()\n",
    "    \n",
    "    # Generate the specified number of 'words'.\n",
    "    for _ in range(words):\n",
    "        # Predict the next word based on the 'model', 'seed' input, and temperature.\n",
    "        logits, word_pred = predict_next_word(model, seed, SEQLEN, total_words, temperature=temperature)\n",
    "        \n",
    "        # Update the 'seed' for the next iteration by adding the predicted word and keeping only the last 10 words.\n",
    "        seed = (seed + [word_pred])[-10:]\n",
    "        \n",
    "        # Append the predicted word to the 'full_text'.\n",
    "        full_text = full_text + [word_pred]\n",
    "    \n",
    "    # Return the generated 'full_text'.\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558a777-a0d6-4390-9211-835c93f48497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate and print 5 paragraphs, each with 12 words, using 'model' and 'test_words' as the initial seed.\n",
    "for _ in range(5):\n",
    "    paragraph = generate_paragraph(model, test_words, 12, None)\n",
    "    print(' '.join(paragraph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7fbbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c62f72c5-1302-4488-911c-aec3c23602c0",
   "metadata": {},
   "source": [
    "## **Randomness through Entropy Scaling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e10933f-1b52-42d4-bd95-15e9fb3ed99e",
   "metadata": {},
   "source": [
    "### What is Entropy in Machine Learning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2bf013c-1564-4fc8-97c0-248e8e736769",
   "metadata": {},
   "source": [
    "## Understanding Entropy in Machine Learning\n",
    "\n",
    "In machine learning, entropy serves as a measure of disorder or uncertainty within a given dataset or system. It quantifies the information content present and plays a significant role in evaluating the effectiveness of models for making accurate predictions. Entropy relies on the concept of probability and is computed using the formula:\n",
    "\n",
    "```\n",
    "Entropy = -Σ (p * log2(p))\n",
    "```\n",
    "\n",
    "Here, \"p\" represents the probability of each possible outcome.\n",
    "\n",
    "### Application in Decision Trees\n",
    "\n",
    "In the context of decision trees, entropy is a valuable tool. It aids in determining the optimal splits at each node of the tree, which, in turn, enhances the model's overall accuracy. This is particularly useful in classification tasks.\n",
    "\n",
    "### An Example: Coin Flipping\n",
    "\n",
    "To grasp the concept of entropy, consider the simple example of flipping a coin. When we flip a coin, there are two potential outcomes: heads or tails. However, due to the inherent randomness involved, it's challenging to precisely predict the outcome. In such situations where the probabilities of each outcome are equal (e.g., 50% heads and 50% tails), entropy is high.\n",
    "\n",
    "Entropy, in this context, reflects the uncertainty or disorder associated with the coin flip and serves as a fundamental concept in machine learning, guiding decisions and predictions in scenarios with varying levels of uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50b3406d-a4f6-4d93-b95a-5c1dfaf0c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06757716, 0.0514937, 0.02583243, 0.023227636, 0.014906152]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(logits, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7b0dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50c079d1-b570-4cfc-b941-753afd9ea5cc",
   "metadata": {},
   "source": [
    "## Softmax Temperature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74382afb-4ef7-4809-930b-c9fb598381fd",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. Temperature scaling has been widely used to improve performance for NLP tasks that utilize the Softmax decision layer.\n",
    "\n",
    "![Example Softmax Image](./images/example_softmax_1.png)\n",
    "<center> \n",
    "  <sub>Image: Example Softmax Distribution</sub> \n",
    "</center>\n",
    "\n",
    "The generated sequence will have a predictable and generic structure. And the reason is less entropy or randomness in the softmax distribution, in the sense that the likelihood of a particular word (corresponding to index 9 in the above example) getting chosen is way higher than the other words. A sequence being predictable is not problematic as long as the aim is to get realistic sequences. But if the goal is to generate a novel text or an image which has never been seen before, randomness is the holy grail.\n",
    "\n",
    "![Temperature vs No Temperature](./images/temp_vs_no_temp.png)\n",
    "<center> \n",
    "  <sub>Image: Impact of Temperature on Softmax Distribution</sub> \n",
    "</center>\n",
    "\n",
    "![Temperature Animation](./images/temp_animation.gif)\n",
    "<center> \n",
    "  <sub>Image: Temperature Animation</sub> \n",
    "</center>\n",
    "\n",
    "The distribution above approaches uniform distribution giving each word an equal probability of getting sampled out, thereby rendering a more creative look to the generated sequence. Too much creativity isn’t good either. In the extreme case, the generated text might not make sense at all. Hence, like all other hyperparameters, this needs to be tuned as well.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "006ac396-9d3a-46ca-a312-488d5773ef7d",
   "metadata": {},
   "source": [
    "## Predicting using the Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1db4279-8aad-48ec-a4ac-a4177a2b8a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the people that walk with their heads downward! the alone court,” to.” pop sits bursting particularly “just conversation. pie—” fairly,” pressing\n",
      "among the people that walk with their heads downward! the i. stole gloves, talk. non-profit trumpet rabbit! pink cartwheels, saucepan ran, forepaws\n",
      "among the people that walk with their heads downward! the hearts, them—“i professor yet invited,” states, room ridiculous (or takes “at hopeful\n",
      "among the people that walk with their heads downward! the pool complying reaching tittered he’ll guard bowed baby; nibbling flat chimneys then—i\n",
      "among the people that walk with their heads downward! the off. broken. by: hearts, doth slates, timidly life! offended!” _proves_ porpoise.” mallets\n"
     ]
    }
   ],
   "source": [
    "# Generate and print 5 paragraphs, each with 12 words, with a temperature of 10.\n",
    "for _ in range(5):\n",
    "    paragraph = generate_paragraph(model, test_words, 12, 10)\n",
    "    print(' '.join(paragraph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16526f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94b82846",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we embarked on an exciting journey into sentiment analysis and text generation using many-to-one LSTMs. We began by delving into sentiment detection, where we trained an LSTM model to analyze airline sentiments based on text reviews. Through meticulous data preprocessing, the use of a bag-of-words representation, and the implementation of the many-to-one LSTM architecture, we achieved impressive accuracy in predicting sentiment labels.\n",
    "\n",
    "Shifting our focus to text generation, we harnessed the timeless literary masterpiece \"Alice's Adventures in Wonderland\" to train our many-to-one LSTMs for generating contextually relevant text. We confronted the challenges of text generation, including dealing with language variability and selecting appropriate words. To fine-tune our text generation, we employed techniques such as entropy scaling and softmax temperature control, allowing us to strike a balance between randomness and diversity in the generated text.\n",
    "\n",
    "Throughout this project, we explored a plethora of concepts and techniques, ranging from the intricacies of preprocessing textual data to the training and evaluation of LSTM models. We developed a deep understanding of sentiment analysis, recognizing its significance in deciphering sentiments within textual data. Furthermore, we delved into the subtleties of text generation, sharpening our skills in next-word prediction and crafting coherent sentences.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d474e81",
   "metadata": {},
   "source": [
    "## **Questions and Answers**\n",
    "\n",
    "1. **Explain the concept of a bag-of-words representation and its role in sentiment analysis.**\n",
    "\n",
    "   A bag-of-words representation is a way to turn text into a list of words without considering their order. It helps in sentiment analysis by counting how often specific words appear in a text. This count can give insights into the overall sentiment, as certain words are associated with positive or negative feelings.\n",
    "\n",
    "2. **How does the preprocessing of textual data help in improving sentiment analysis results?**\n",
    "\n",
    "   Text data preprocessing involves tasks like removing punctuation and lowercasing words. It helps by making the text more consistent and removing noise, which can lead to better sentiment analysis results.\n",
    "\n",
    "3. **Describe the steps involved in training an LSTM model for sentiment analysis.**\n",
    "\n",
    "   Training an LSTM for sentiment analysis involves three main steps: data preparation (like turning text into numbers), model building (creating the LSTM architecture), and training (teaching the model to predict sentiment based on the data).\n",
    "\n",
    "4. **What is the purpose of the softmax activation function in the output layer of the LSTM model?**\n",
    "\n",
    "   The softmax function assigns probabilities to different sentiment classes. It helps the model decide which sentiment category the input text most likely belongs to.\n",
    "\n",
    "5. **How do you evaluate the performance of an LSTM model in sentiment analysis?**\n",
    "\n",
    "   Model performance is assessed using metrics like accuracy, precision, recall, and F1-score. These metrics measure how well the model predicts sentiment labels.\n",
    "\n",
    "6. **Explain the challenges associated with text generation using many-to-one LSTMs.**\n",
    "\n",
    "   Challenges in text generation include making text sound coherent, avoiding repetition, and generating contextually relevant content. Many-to-one LSTMs must overcome these hurdles to create meaningful text.\n",
    "\n",
    "7. **What role does data preprocessing play in text generation using many-to-one LSTMs?**\n",
    "\n",
    "   Data preprocessing in text generation helps in cleaning and structuring the input text. This ensures that the model receives high-quality data, leading to better generated text.\n",
    "\n",
    "8. **Describe the steps involved in generating new text using a trained LSTM model.**\n",
    "\n",
    "   To generate text, the model starts with a seed word or phrase. It predicts the next word based on the seed and continues predicting words until the desired length is reached, creating a coherent sequence.\n",
    "\n",
    "9. **What is the purpose of entropy scaling in text generation? How does it impact the generated text?**\n",
    "\n",
    "   Entropy scaling adjusts the randomness in text generation. Higher entropy means more randomness, potentially leading to incoherent text. Lower entropy makes the text more deterministic and less creative.\n",
    "\n",
    "10. **Explain the concept of softmax temperature and its role in controlling prediction randomness.**\n",
    "\n",
    "    Softmax temperature controls the diversity of predictions. A higher temperature leads to more randomness, while a lower temperature results in more deterministic predictions.\n",
    "\n",
    "11. **How can you ensure that the generated text is contextually relevant and coherent?**\n",
    "\n",
    "    Ensuring contextually relevant and coherent text involves training on a large, diverse dataset and using techniques like attention mechanisms to help the model focus on relevant context.\n",
    "\n",
    "12. **What techniques can be used to improve the diversity and creativity of the generated text?**\n",
    "\n",
    "    Techniques like adjusting the softmax temperature, increasing the model's complexity, and fine-tuning the training process can enhance text diversity and creativity.\n",
    "\n",
    "13. **Discuss the trade-off between randomness and coherence in text generation using LSTMs.**\n",
    "\n",
    "    Balancing randomness and coherence is challenging. More randomness can lead to creative but incoherent text, while more coherence may make the text less diverse. It's a trade-off that needs careful tuning.\n",
    "\n",
    "14. **How can you handle out-of-vocabulary words when training an LSTM model for text generation?**\n",
    "\n",
    "    Handling out-of-vocabulary words involves adding them to the training data or using techniques like subword tokenization to ensure the model can generate text containing those words.\n",
    "\n",
    "15. **What are the limitations of using many-to-one LSTMs for text generation?**\n",
    "\n",
    "    Many-to-one LSTMs might struggle with capturing long-term dependencies in text. They can also produce text that is repetitive or lacks detailed context.\n",
    "\n",
    "16. **How can you handle long-term dependencies in text generation using LSTMs?**\n",
    "\n",
    "    Long-term dependencies can be addressed by using more advanced LSTM architectures, like those with attention mechanisms, or by training on more extensive datasets to capture a broader context.\n",
    "\n",
    "17. **Discuss the impact of different hyperparameters on the performance of LSTM models in text generation.**\n",
    "\n",
    "    Hyperparameters, like the learning rate and the model's complexity, greatly affect text generation. Finding the right balance is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0d034",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306012ae-f900-4e28-963c-12cd527b9ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
